{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Chaos Expansion example: Hyperbolic truncation and Least Angle Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Lukas Novak, Dimitrios Loukrezis \\ \n",
    "Date: February 28 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we approximate the well-known Ishigami function with a total-degree Polynomial Chaos Expansion further reduced by hyperbolic truncation. In order to reduce the number of basis functions, we use the best-model selection algorithm based on Least Angle Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "from UQpy.distributions import Uniform, JointIndependent\n",
    "from UQpy.surrogates import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We then define the Ishigami function, which reads:\n",
    "$$f(x_1, x_2, x_3) = \\sin(x_1) + a \\sin^2(x_2) + b x_3^4 \\sin(x_1).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to be approximated\n",
    "def ishigami(xx):\n",
    "    \"\"\"Ishigami function\"\"\"\n",
    "    a = 7\n",
    "    b = 0.1\n",
    "    term1 = np.sin(xx[0])\n",
    "    term2 = a * np.sin(xx[1])**2\n",
    "    term3 = b * xx[2]**4 * np.sin(xx[0])\n",
    "    return term1 + term2 + term3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ishigami function has three indepdent random inputs, which are uniformly distributed in interval $[-\\pi, \\pi]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input distributions\n",
    "dist1 = Uniform(loc=-np.pi, scale=2*np.pi)    \n",
    "dist2 = Uniform(loc=-np.pi, scale=2*np.pi)\n",
    "dist3 = Uniform(loc=-np.pi, scale=2*np.pi)    \n",
    "marg = [dist1, dist2, dist3]\n",
    "joint = JointIndependent(marginals=marg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our complete PCE, which will be further used for the best model selection algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now select a polynomial basis. Here we opt for a total-degree (TD) basis, such that the univariate polynomials have a maximum degree equal to $P$ and all multivariate polynomial have a total-degree (sum of degrees of corresponding univariate polynomials) at most equal to $P$. The size of the basis is then given by \n",
    "$$\\frac{(N+P)!}{N! P!},$$\n",
    "where $N$ is the number of random inputs (here, $N=3$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the size of the basis is highly dependent both on $N$ and $P$. It is generally advisable that the experimental design has $2-10$ times more data points than the number of PCE polynomials. This might lead to curse of dimensionality and thus we will utilize the best model selection algorithm based on Least Angle Regerssion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum polynomial degree\n",
    "P = 15  \n",
    "# construct total-degree polynomial basis\n",
    "polynomial_basis = PolynomialBasis.create_total_degree_basis(joint,P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now compute the PCE coefficients. For that we first need a training sample of input random variable realizations and the corresponding model outputs. These two data sets form what is also known as an ''experimental design''. In case of adaptive construction of PCE by the best model selection algorithm, size of ED is given apriori and the most suitable basis functions are adaptively selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of experimental design: 500\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "sample_size = 500\n",
    "print('Size of experimental design:', sample_size)\n",
    "\n",
    "# realizations of random inputs\n",
    "xx_train = joint.rvs(sample_size)\n",
    "# corresponding model outputs\n",
    "yy_train = np.array([ishigami(x) for x in xx_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the PCE coefficients by solving a regression problem. Here we opt for the _np.linalg.lstsq_ method, which is based on the _dgelsd_ solver of LAPACK. This original PCE class will be used for further selection of the best basis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "least_squares = LeastSquareRegression()\n",
    "pce = PolynomialChaosExpansion(polynomial_basis=polynomial_basis, regression_method=least_squares)\n",
    "pce.fit(xx_train, yy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created the PCE containing all basis functions generated by TD algorithm, it is possible to reduce the number of basis functions by LAR algorithm. The best model selection algorithm in UQPy is based on results of LAR adding basis functions to active set one-by-one until the target accuracy is obtained. Approximation error is measured by leave-one-out error $Q^2$ on given ED in [0,1]. Target error represents the target accuracy measured by $Q^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if the target error is too high (close to 1), there is a risk of over-fitting. Therefore, we must check the over-fitting by empirical rule: if the three steps of LAR in row lead to decreasing accuracy - stop the algorithm. It is recommended to always check the over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the full set of PCE basis: 816\n",
      "Size of the LAR PCE basis: 20\n"
     ]
    }
   ],
   "source": [
    "# check the size of the basis\n",
    "print('Size of the full set of PCE basis:', polynomial_basis.polynomials_number)\n",
    "\n",
    "target_error=1\n",
    "CheckOverfitting = True\n",
    "pceLAR=polynomial_chaos.regressions.LeastAngleRegression.model_selection(pce,target_error,CheckOverfitting)\n",
    "\n",
    "print('Size of the LAR PCE basis:', pceLAR.polynomial_basis.polynomials_number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simply post-processing the PCE's terms, we are able to get estimates regarding the mean and standard deviation of the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCE mean estimate: 3.5\n",
      "PCE variance estimate: 13.8442\n"
     ]
    }
   ],
   "source": [
    "mean_est,var_est=pceLAR.get_moments(higher=False)\n",
    "print('PCE mean estimate:', mean_est)\n",
    "print('PCE variance estimate:', var_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to obtain skewness and kurtosis (3rd and 4th moments), though it might be computationally demanding for high $N$ and $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCE mean estimate: 3.5000270645111775\n",
      "PCE variance estimate: 13.844159774279857\n",
      "PCE skewness estimate: 1.1399056234656551e-05\n",
      "PCE kurtosis estimate: 3.5073685859514554\n"
     ]
    }
   ],
   "source": [
    "mean_est,var_est,skew_est,kurt_est=pceLAR.get_moments(True)\n",
    "print('PCE mean estimate:', mean_est)\n",
    "print('PCE variance estimate:', var_est)\n",
    "print('PCE skewness estimate:', skew_est)\n",
    "print('PCE kurtosis estimate:', kurt_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the statistical moments, we can very simply estimate the Sobol sensitivity indices, which quantify the importance of the input random variables in terms of impact on the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-order Sobol indices:\n",
      "[[0.31390058]\n",
      " [0.44242378]\n",
      " [0.        ]]\n",
      "Total-order Sobol indices:\n",
      "[[0.55757332]\n",
      " [0.44242378]\n",
      " [0.24367274]]\n"
     ]
    }
   ],
   "source": [
    "from UQpy.sensitivity import *\n",
    "pce_sensitivity = PceSensitivity(pceLAR)\n",
    "sobol_first = pce_sensitivity.first_order_indices()\n",
    "sobol_total = pce_sensitivity.total_order_indices()\n",
    "print('First-order Sobol indices:')\n",
    "print(sobol_first)\n",
    "print('Total-order Sobol indices:')\n",
    "print(sobol_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of PCE is typically measured by leave-one-out error $Q^2$ on given ED. Moreover, we will test that also by computing the mean absolute error (MAE) between the PCE's predictions and the true model outputs, given a validation sample of $10^5$ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.00047330185528459475\n",
      "Leave-one-out cross validation on ED: 0.0\n"
     ]
    }
   ],
   "source": [
    "n_samples_val = 100000\n",
    "xx_val = joint.rvs(n_samples_val)\n",
    "yy_val = np.array([ishigami(x) for x in xx_val])\n",
    "\n",
    "yy_val_pce = pceLAR.predict(xx_val).flatten()\n",
    "errors = np.abs(yy_val.flatten() - yy_val_pce)\n",
    "MAE=(np.linalg.norm(errors, 1)/n_samples_val)\n",
    "\n",
    "print('Mean absolute error:', MAE)\n",
    "print('Leave-one-out cross validation on ED:', pceLAR.leaveoneout_error())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the comparison, we can check results of PCE solved by OLS without the model selection algorithm. Note that, it is necessary to use 2âˆ’10 times more data points than the number of PCE polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of ED: 5\n",
      "Polynomial degree: 0\n",
      "Mean absolute error: 3.5092014513593974\n",
      " \n",
      "Size of ED: 20\n",
      "Polynomial degree: 1\n",
      "Mean absolute error: 2.9190940130421446\n",
      " \n",
      "Size of ED: 50\n",
      "Polynomial degree: 2\n",
      "Mean absolute error: 2.881426057510191\n",
      " \n",
      "Size of ED: 100\n",
      "Polynomial degree: 3\n",
      "Mean absolute error: 2.490517639729513\n",
      " \n",
      "Size of ED: 175\n",
      "Polynomial degree: 4\n",
      "Mean absolute error: 1.6837629839128996\n",
      " \n",
      "Size of ED: 280\n",
      "Polynomial degree: 5\n",
      "Mean absolute error: 1.4288047926039877\n",
      " \n",
      "Size of ED: 420\n",
      "Polynomial degree: 6\n",
      "Mean absolute error: 0.47225689340879845\n",
      " \n",
      "Size of ED: 600\n",
      "Polynomial degree: 7\n",
      "Mean absolute error: 0.3275845356142157\n",
      " \n",
      "Size of ED: 825\n",
      "Polynomial degree: 8\n",
      "Mean absolute error: 0.06852103811265521\n",
      " \n",
      "Size of ED: 1100\n",
      "Polynomial degree: 9\n",
      "Mean absolute error: 0.04424218398187157\n",
      " \n",
      "Size of ED: 1430\n",
      "Polynomial degree: 10\n",
      "Mean absolute error: 0.004973204109526657\n",
      " \n",
      "Size of ED: 1820\n",
      "Polynomial degree: 11\n",
      "Mean absolute error: 0.0038931663654603594\n",
      " \n",
      "Size of ED: 2275\n",
      "Polynomial degree: 12\n",
      "Mean absolute error: 0.00025446069874232206\n",
      " \n",
      "Size of ED: 2800\n",
      "Polynomial degree: 13\n",
      "Mean absolute error: 0.00025430428526723004\n",
      " \n",
      "Size of ED: 3400\n",
      "Polynomial degree: 14\n",
      "Mean absolute error: 1.118914789946593e-05\n",
      " \n",
      "Size of ED: 4080\n",
      "Polynomial degree: 15\n",
      "Mean absolute error: 1.071680855024345e-05\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# validation data sets\n",
    "np.random.seed(999) # fix random seed for reproducibility\n",
    "n_samples_val = 100000\n",
    "xx_val = joint.rvs(n_samples_val)\n",
    "yy_val = np.array([ishigami(x) for x in xx_val])\n",
    "\n",
    "mae = [] # to hold MAE for increasing polynomial degree\n",
    "for degree in range(16):\n",
    "    # define PCE\n",
    "    polynomial_basis = PolynomialBasis.create_total_degree_basis(joint, degree)\n",
    "    least_squares = LeastSquareRegression()\n",
    "    pce_metamodel = PolynomialChaosExpansion(polynomial_basis=polynomial_basis, regression_method=least_squares)\n",
    "       \n",
    "    # create training data\n",
    "    np.random.seed(1) # fix random seed for reproducibility\n",
    "    sample_size = int(pce_metamodel.polynomials_number*5)\n",
    "    xx_train = joint.rvs(sample_size)\n",
    "    yy_train = np.array([ishigami(x) for x in xx_train])\n",
    "    \n",
    "    # fit PCE coefficients\n",
    "    pce_metamodel.fit(xx_train, yy_train)\n",
    "    \n",
    "    # compute mean absolute validation error\n",
    "    yy_val_pce = pce_metamodel.predict(xx_val).flatten()\n",
    "    errors = np.abs(yy_val.flatten() - yy_val_pce)\n",
    "    mae.append(np.linalg.norm(errors, 1)/n_samples_val)\n",
    "    print('Size of ED:',sample_size)\n",
    "    print('Polynomial degree:', degree)\n",
    "    print('Mean absolute error:', mae[-1])\n",
    "    print(' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of high-dimensional input and/or high $P$ it is also beneficial to reduce the TD basis set by hyperbolic trunction. The hyperbolic truncation reduces higher-order interaction terms in dependence to parameter $q$ in interval (0,1). The set of multi indices $\\boldsymbol{\\alpha}$ is reduced as follows:\n",
    "\\begin{equation}\n",
    "        {\\boldsymbol{\\alpha}}\\in \\mathbb{N}^{N} : \n",
    "        || \\boldsymbol{\\alpha}||_q \n",
    "        \\equiv \n",
    "        \\Big( \\sum_{i=1}^{N} \\alpha_i^q \\Big)^{1/q}  \n",
    "        \\leq P \n",
    "\\end{equation}\n",
    "\n",
    "Note that $q=1$ leads to full TD set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the full set of PCE basis: 816\n",
      "Size of the hyperbolic full set of PCE basis: 442\n"
     ]
    }
   ],
   "source": [
    "print('Size of the full set of PCE basis:', PolynomialBasis.create_total_degree_basis(joint,P).polynomials_number)\n",
    "q=0.8\n",
    "polynomial_basis_hyperbolic = PolynomialBasis.create_total_degree_basis(joint,P,q)\n",
    "# check the size of the basis\n",
    "print('Size of the hyperbolic full set of PCE basis:', polynomial_basis_hyperbolic.polynomials_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction of the full set size significantly reduces the necessary number of data points in ED for non-adaptive PCE. However, it suitable only for mathematical models without significant higher-order interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 9.160937802983538e-05\n",
      "Leave-one-out cross validation on ED: 0.0\n"
     ]
    }
   ],
   "source": [
    "pce = PolynomialChaosExpansion(polynomial_basis=polynomial_basis_hyperbolic, regression_method=least_squares)\n",
    "pce.fit(xx_train, yy_train)\n",
    "yy_val_pce = pce.predict(xx_val).flatten()\n",
    "errors = np.abs(yy_val.flatten() - yy_val_pce)\n",
    "MAE=(np.linalg.norm(errors, 1)/n_samples_val)\n",
    "\n",
    "print('Mean absolute error:', MAE)\n",
    "print('Leave-one-out cross validation on ED:', pce.leaveoneout_error())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

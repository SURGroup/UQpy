{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Model Selection\n",
    "\n",
    "Authors: Yuchen Zhou and Audrey Olivier - 12/10/2018 <br>\n",
    "Last modified by Audrey Olivier on 05/15/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick review of Bayesian model selection\n",
    "\n",
    "The problem of model selection consists in determining which model(s) best explain the available data $D$, given a set of candidate models $m_{1:M}$. Each model $m_{j}$ is parameterized by a set of parameters $\\theta_{m_{j}} \\in \\Theta_{m_{j}}$, to be estimated based on data. In the Bayesian framework, model selection is perfomed by computing the posterior probability of each model $m_{j}$ using Bayes' theorem:\n",
    "\n",
    "$$P(m_{j} \\vert D) = \\frac{p(D \\vert m_{j})P(m_{j})}{\\sum_{j=1}^{M} P(D \\vert m_{j})P(m_{j})}$$\n",
    "\n",
    "where $P(m_{j})$ is the prior assigned to model $m_{j}$ and $P(D \\vert m_{j})$ is the model evidence, also called marginal likelihood.\n",
    "\n",
    "$$ p(D \\vert m_{j}) = \\int_{\\Theta_{m_{j}}} p(D \\vert m_{j}, \\theta_{m_{j}}) p(\\theta_{m_{j}} \\vert m_{j}) d\\theta_{m_{j}} $$\n",
    "\n",
    "where $p(\\theta_{m_{j}} \\vert m_{j})$ is the prior assigned to the parameter vector of model $m_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical example\n",
    "\n",
    "In the following we present an example for which the posterior pdf of the parameters, evidences and model probabilities can be computed analytically. We drop the $m_{j}$ subscript when referring to model parameters for simplicity. Three models are considered (the domain $x$ is fixed and consists in 50 equally spaced points):\n",
    "\\begin{align*}\n",
    "m_{linear}:& \\quad y = \\theta_{0} x + \\epsilon \\\\\n",
    "m_{quadratic}:& \\quad y = \\theta_{0} x + \\theta_{1} x^2 + \\epsilon \\\\\n",
    "m_{cubic}:& \\quad y = \\theta_{0} x + \\theta_{1} x^2+ \\theta_{2} x^3 + \\epsilon \\\\\n",
    "\\end{align*}\n",
    "\n",
    "All three models can be written in a compact form as $y=X \\theta + \\epsilon$, where $X$ contains the necessary powers of $x$. For all three models, the prior is chosen to be Gaussian, $p(\\theta) = N(\\cdot, \\theta_{prior}, \\Sigma_{prior})  $, and so is the noise $\\epsilon \\sim N(\\cdot; 0, \\sigma_{n}^{2} I)$. Then the posterior of the parameters can be computed analytically as:\n",
    "\n",
    "\\begin{align*}\n",
    "& p(\\theta \\vert D={x,y}) =  N(\\cdot; \\theta_{post}(D), \\Sigma_{post}(D)) \\\\\n",
    "& \\theta_{post}(D) = \\left( \\frac{1}{\\sigma_{n}^{2}}X^{T}X + \\Sigma_{prior}^{-1} \\right)^{-1} \\left(\\frac{1}{\\sigma_{n}^{2}}X^{T}y+\\Sigma^{-1}\\theta_{prior} \\right) \\\\\n",
    "& \\Sigma_{post}(D) = \\left( \\frac{1}{\\sigma_{n}^{2}}X^{T}X + \\Sigma_{prior}^{-1} \\right)^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "Then the evidence of each model can be computed as \n",
    "\n",
    "$$ p(D) = \\frac{p(D \\vert \\theta)p(\\theta)}{p(\\theta \\vert D)} $$\n",
    "where $p(D \\vert \\theta) = N(\\cdot; X\\theta, \\sigma_{n}^{2} I)$, $p(\\theta) = N(\\cdot, \\theta_{prior}, \\Sigma_{prior}) $ and $p(\\theta \\vert D) = N(\\cdot, \\theta_{post}(D), \\Sigma_{post}(D))$. This formula can be computed at any point $\\theta$.\n",
    "\n",
    "### Generate data from the quadratic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dimitris/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from UQpy.Inference import *\n",
    "from UQpy.RunModel import RunModel # required to run the quadratic model\n",
    "from sklearn.neighbors import KernelDensity # for the plots\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "from UQpy.Distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "[ -0.6681285   -0.21082927   1.35993359   1.93062478   3.49961402\n",
      "   4.73246234   4.52520653   5.95968726   6.61795398   8.26869254\n",
      "   8.35470754  11.19380042  13.33213561  16.96838006  18.8585438\n",
      "  23.74811764  26.5054303   28.4630389   31.5986224   35.10590689\n",
      "  37.29487518  40.93165653  43.83484551  48.37478999  54.0178402\n",
      "  57.30005362  61.24904528  65.93381664  71.12804783  77.11414629\n",
      "  79.78168287  86.10212889  92.69324162  96.00700906 104.55678389\n",
      " 108.35056844 115.97225104 121.67318536 128.39330449 133.44389061\n",
      " 141.56142567 148.03345624 156.7101402  162.25358506 170.23398557\n",
      " 177.57212657 186.03681389 194.80013151 201.7321561  210.44543202]\n"
     ]
    }
   ],
   "source": [
    "# Generate data from a quadratic function\n",
    "param_true = np.array([1.0, 2.0]).reshape(1, -1)\n",
    "var_n = 1\n",
    "error_covariance = var_n * np.eye(50)\n",
    "print(param_true.shape)\n",
    "\n",
    "z = RunModel(samples=param_true, model_script='pfn_models.py', model_object_name = 'model_quadratic', vec=False,\n",
    "             var_names = ['theta_1', 'theta_2'])\n",
    "data_clean = z.qoi_list[0].reshape((-1,))\n",
    "data = data_clean + Normal(scale=np.sqrt(var_n)).rvs(nsamples=data_clean.size, random_state=456).reshape((-1,))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the models, compute the true values of the evidence.\n",
    "\n",
    "For all three models, a Gaussian prior is chosen for the parameters, with mean and covariance matrix of the appropriate dimensions. Each model is given prior probability $P(m_{j}) = 1/3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "model_names = ['model_linear', 'model_quadratic', 'model_cubic']\n",
    "model_n_params = [1, 2, 3]\n",
    "model_prior_means = [[0.], [0., 0.], [0., 0., 0.]]\n",
    "model_prior_stds = [[10.], [1., 1.], [1., 2., 0.25]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior mean and covariance for model_linear\n",
      "[16.16834799] [[0.00059394]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'multivariate_normal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2dd66542204e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# compute evidence, evaluate the formula at the posterior mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mlike_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm_posterior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_covariance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprior_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_posterior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mS_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mposterior_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_posterior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm_posterior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mS_posterior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multivariate_normal' is not defined"
     ]
    }
   ],
   "source": [
    "evidences = []\n",
    "model_posterior_means = []\n",
    "model_posterior_stds = []\n",
    "for n, model in enumerate(model_names):\n",
    "    # compute matrix X\n",
    "    X = np.linspace(0, 10, 50).reshape((-1,1))\n",
    "    if n == 1: # quadratic model\n",
    "        X = np.concatenate([X, X**2], axis=1)\n",
    "    if n == 2: # cubic model\n",
    "        X = np.concatenate([X, X**2, X**3], axis=1)\n",
    "\n",
    "    # compute posterior pdf\n",
    "    m_prior = np.array(model_prior_means[n]).reshape((-1,1))\n",
    "    S_prior = np.diag(np.array(model_prior_stds[n])**2)\n",
    "    S_posterior = np.linalg.inv(1/var_n*np.matmul(X.T,X)+np.linalg.inv(S_prior))\n",
    "    m_posterior = np.matmul(S_posterior, \n",
    "                            1/var_n*np.matmul(X.T, data.reshape((-1,1)))+np.matmul(np.linalg.inv(S_prior),m_prior))\n",
    "    m_prior = m_prior.reshape((-1,))\n",
    "    m_posterior = m_posterior.reshape((-1,))\n",
    "    model_posterior_means.append(list(m_posterior))\n",
    "    model_posterior_stds.append(list(np.sqrt(np.diag(S_posterior))))\n",
    "    print('posterior mean and covariance for '+model)\n",
    "    print(m_posterior, S_posterior)\n",
    "    \n",
    "    # compute evidence, evaluate the formula at the posterior mean\n",
    "    like_theta = multivariate_normal.pdf(data, mean=np.matmul(X,m_posterior).reshape((-1,)), cov=error_covariance)\n",
    "    prior_theta = multivariate_normal.pdf(m_posterior, mean=m_prior, cov=S_prior)\n",
    "    posterior_theta = multivariate_normal.pdf(m_posterior, mean=m_posterior, cov=S_posterior)\n",
    "    evidence = like_theta*prior_theta/posterior_theta\n",
    "    evidences.append(evidence)\n",
    "    print('evidence for '+model+'= {}\\n'.format(evidence))\n",
    "    \n",
    "# compute the posterior probability of each model\n",
    "tmp = [1/3*evidence for evidence in evidences]\n",
    "model_posterior_probas = [p/sum(tmp) for p in tmp]\n",
    "\n",
    "print('posterior probabilities of all three models')\n",
    "print(model_posterior_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the models for use in UQpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "from UQpy.Distributions import Normal, JointInd\n",
    "candidate_models = []\n",
    "for n, model_name in enumerate(model_names):\n",
    "    run_model = RunModel(model_script='pfn_models.py', model_object_name=model_name, vec=False)\n",
    "    prior = JointInd([Normal(loc=m, scale=std) for m, std in zip(model_prior_means[n], model_prior_stds[n])])\n",
    "    model = InferenceModel(\n",
    "        nparams=model_n_params[n], runmodel_object=run_model, prior=prior, error_covariance=error_covariance, \n",
    "        name=model_name)\n",
    "    candidate_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MCMC for one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic model\n",
    "from UQpy.SampleMethods import MH\n",
    "bayesMCMC = BayesParameterEstimation(\n",
    "    data=data, inference_model=candidate_models[1], sampling_class=MH, nsamples=3500, jump=10, nburn=100, \n",
    "    proposal=JointInd([Normal(scale=0.1), ] * 2), seed=[0., 0.], verbose=True, random_state=123)\n",
    "# plot prior, true posterior and estimated posterior\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,5))\n",
    "for n_p in range(2):\n",
    "    domain_plot = np.linspace(-0.5,3,200)\n",
    "    ax[n_p].plot(domain_plot, norm.pdf(domain_plot, loc=model_prior_means[1][n_p], scale=model_prior_stds[1][n_p]),\n",
    "                 label='prior', color='green', linestyle='--')\n",
    "    ax[n_p].plot(domain_plot, norm.pdf(domain_plot, loc=model_posterior_means[1][n_p], \n",
    "                                      scale=model_posterior_stds[1][n_p]),\n",
    "                 label='true posterior', color='red', linestyle='-')\n",
    "    ax[n_p].hist(bayesMCMC.sampler.samples[:, n_p], density=True, bins=30, label='estimated posterior MCMC')\n",
    "    ax[n_p].legend()\n",
    "    ax[n_p].set_title('MCMC for quadratic model')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Bayesian Model Selection for all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines constants for the MCMC learning part, same as above\n",
    "proposals = [Normal(scale=0.1),\n",
    "             JointInd([Normal(scale=0.1), Normal(scale=0.1)]),\n",
    "             JointInd([Normal(scale=0.15), Normal(scale=0.1), Normal(scale=0.05)])]\n",
    "nsamples = [2000, 6000, 14000]\n",
    "nburn = [500, 2000, 4000]\n",
    "jump = [5, 10, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = BayesModelSelection(\n",
    "    data=data, candidate_models=candidate_models, prior_probabilities=[1./3., 1./3., 1./3.],\n",
    "    nsamples=nsamples, sampling_class=[MH, ]*3, jump=jump, nburn=nburn, proposal=proposals,\n",
    "    seed=model_prior_means, verbose=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(selection.probabilities)[::-1]\n",
    "print('Sorted models:')\n",
    "print([selection.candidate_models[i].name for i in sorted_indices])\n",
    "print('Evidence of sorted models:')\n",
    "print([selection.evidences[i] for i in sorted_indices])\n",
    "print('Posterior probabilities of sorted models:')\n",
    "print([selection.probabilities[i] for i in sorted_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of version 2, the implementation of BayesModelSelection in UQpy uses the method of the harmonic mean to compute the models' evidence. This method is known to behave quite poorly, in particular it yeidls estimates with large variance. In the problem above, this implementation does not consistently detects that the quadratic model has the highest model probability. Future versions of UQpy will integrate more advanced methods for the estimation of the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (m, be) in enumerate(zip(selection.candidate_models, selection.bayes_estimators)):\n",
    "    # plot prior, true posterior and estimated posterior\n",
    "    print('Posterior parameters for model '+m.name)\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(16,5))\n",
    "    for n_p in range(m.nparams):\n",
    "        domain_plot = np.linspace(min(be.sampler.samples[:, n_p]), max(be.sampler.samples[:, n_p]), 200)\n",
    "        ax[n_p].plot(domain_plot, norm.pdf(domain_plot, loc=model_prior_means[i][n_p], \n",
    "                                           scale=model_prior_stds[i][n_p]),\n",
    "                    label = 'prior', color='green', linestyle='--')\n",
    "        ax[n_p].plot(domain_plot, norm.pdf(domain_plot, loc=model_posterior_means[i][n_p], \n",
    "                                           scale=model_posterior_stds[i][n_p]),\n",
    "                    label = 'true posterior', color='red', linestyle='-')\n",
    "        ax[n_p].hist(be.sampler.samples[:, n_p], density=True, bins=30, label='estimated posterior MCMC')\n",
    "        ax[n_p].legend()\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
